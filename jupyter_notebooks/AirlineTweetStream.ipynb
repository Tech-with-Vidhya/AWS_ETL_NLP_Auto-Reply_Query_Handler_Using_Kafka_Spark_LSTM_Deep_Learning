{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "04YVxele9mHl"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-y4z7NpO93Cq"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GCtpXdox96VS"
   },
   "outputs": [],
   "source": [
    "#Tweet preprocessing and sentiment analysis\n",
    "#Import the necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import tensorflow.keras.models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#Text Preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import re\n",
    "import string\n",
    "from nltk import PorterStemmer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonSchema = StructType([ StructField(\"tweet\", StringType(), True), StructField(\"user\", StringType(), True), StructField(\"tweet_id\", StringType(), True) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"TwitterSupportML/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BinaryInference:\n",
    "\n",
    "      def __init__(self):\n",
    "          self.load_models()\n",
    "        \n",
    "      def clean_text(self, txt):\n",
    "\n",
    "          \"\"\"\n",
    "          removing all hashtags , punctuations, stop_words  and links, also stemming words \n",
    "          \"\"\"\n",
    "          from nltk.corpus import stopwords\n",
    "          txt = txt.lower()\n",
    "          txt = re.sub(r\"(?<=\\w)nt\", \"not\",txt) #change don't to do not cna't to cannot \n",
    "          txt = re.sub(r\"(@\\S+)\", \"\", txt)  # remove hashtags\n",
    "          txt = re.sub(r'\\W', ' ', str(txt)) # remove all special characters including apastrophie \n",
    "          txt = txt.translate(str.maketrans('', '', string.punctuation)) # remove punctuations \n",
    "          txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)   # remove all single characters (it's -> it s then we need to remove s)\n",
    "          txt = re.sub(r'\\s+', ' ', txt, flags=re.I) # Substituting multiple spaces with single space\n",
    "          txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links \n",
    "          txt = ' '.join([PorterStemmer().stem(word=word) for word in txt.split(\" \") if word not in stopwords.words('english') ]) # stem & remove stop words\n",
    "          txt = ''.join([i for i in txt if not i.isdigit()]).strip() # remove digits ()\n",
    "          return txt\n",
    "\n",
    "      def get_model(self):\n",
    "          max_fatures = 2000\n",
    "          embed_dim = 128\n",
    "          lstm_out = 196\n",
    "          input_len = 32\n",
    "          model = Sequential()\n",
    "          model.add(Embedding(max_fatures, embed_dim,input_length = input_len))\n",
    "          model.add(SpatialDropout1D(0.4))\n",
    "          model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "          model.add(Dense(2,activation='softmax'))\n",
    "          model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "          return model\n",
    "\n",
    "\n",
    "      def load_models(self):\n",
    "          with open(ROOT_DIR+'model_binaryclass/tokenizerBinaryClassification.pickle', 'rb') as handle:\n",
    "              self.tokenizer = pickle.load(handle)\n",
    "\n",
    "          self.model = self.get_model()\n",
    "          self.model.load_weights(ROOT_DIR+\"model_binaryclass/binaryClassificationModel.h5\")\n",
    "\n",
    "\n",
    "      def predict_complaint(self, text):\n",
    "\n",
    "          #vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "          text = self.clean_text(text)\n",
    "          twt = self.tokenizer.texts_to_sequences([text])\n",
    "          #padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "          twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "          complain = self.model.predict(twt,batch_size=1,verbose = 0)[0]\n",
    "          if(np.argmax(complain) == 0):\n",
    "              print(\"negative\")\n",
    "              return True\n",
    "          elif (np.argmax(complain) == 1):\n",
    "              print(\"positive\")\n",
    "              return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complaint Category\n",
    "\n",
    "class MulticlassComplainInference:\n",
    "\n",
    "      def __init__(self):\n",
    "          self.load_models()\n",
    "\n",
    "      def get_model(self):\n",
    "          # The maximum number of words to be used. (most frequent)\n",
    "          MAX_NB_WORDS = 50000\n",
    "          # Max number of words in each complaint.\n",
    "          MAX_SEQUENCE_LENGTH = 250\n",
    "          # This is fixed.\n",
    "          EMBEDDING_DIM = 100\n",
    "          model = Sequential()\n",
    "          model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "          model.add(SpatialDropout1D(0.2))\n",
    "          model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "          model.add(Dense(8, activation='softmax'))\n",
    "          model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "          return model\n",
    "\n",
    "\n",
    "      def load_models(self):\n",
    "          with open(ROOT_DIR+'model_multiclass/tokenizerMulticlassComplaintClassification.pickle', 'rb') as handle:\n",
    "              self.tokenizer = pickle.load(handle)\n",
    "\n",
    "          self.model = self.get_model()\n",
    "          self.model.load_weights(ROOT_DIR+\"model_multiclass/multiclassComplaintClassifier.h5\")\n",
    "\n",
    "      def clean_text(self, txt):\n",
    "\n",
    "          \"\"\"\n",
    "          removing all hashtags , punctuations, stop_words  and links, also stemming words \n",
    "          \"\"\"\n",
    "          from nltk.corpus import stopwords\n",
    "          txt = txt.lower()\n",
    "          txt = re.sub(r\"(?<=\\w)nt\", \"not\",txt) #change don't to do not cna't to cannot \n",
    "          txt = re.sub(r\"(@\\S+)\", \"\", txt)  # remove hashtags\n",
    "          txt = re.sub(r'\\W', ' ', str(txt)) # remove all special characters including apastrophie \n",
    "          txt = txt.translate(str.maketrans('', '', string.punctuation)) # remove punctuations \n",
    "          txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)   # remove all single characters (it's -> it s then we need to remove s)\n",
    "          txt = re.sub(r'\\s+', ' ', txt, flags=re.I) # Substituting multiple spaces with single space\n",
    "          txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links \n",
    "          txt = ' '.join([PorterStemmer().stem(word=word) for word in txt.split(\" \") if word not in stopwords.words('english') ]) # stem & remove stop words\n",
    "          txt = ''.join([i for i in txt if not i.isdigit()]).strip() # remove digits ()\n",
    "          return txt\n",
    "\n",
    "      def predict_complaint_type(self, text):\n",
    "          MAX_SEQUENCE_LENGTH = 250\n",
    "          new_tweet = self.clean_text(text)\n",
    "          seq = self.tokenizer.texts_to_sequences([new_tweet])\n",
    "          padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "          pred = self.model.predict(padded)\n",
    "          labels = ['Reschedule and Refund', 'Baggage Issue','Phone and Online Booking', 'Extra Charges', \n",
    "                    'Delay and Customer Service', 'Seating Preferences', 'Reservation Issue', 'Customer Experience']\n",
    "          print(pred, labels[np.argmax(pred)])\n",
    "          return labels[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameEntities:\n",
    "\n",
    "      def __init__(self):\n",
    "\n",
    "          # ROOT_DIR = \"\"\n",
    "          self.nlp = en_core_web_sm.load() # Load the saved model and predict\n",
    "          output_dir = Path(ROOT_DIR+'model_NER/')\n",
    "          print(\"Loading from\", output_dir)\n",
    "          self.nlp_updated = spacy.load(output_dir)\n",
    "\n",
    "      def clean_text(self, txt):\n",
    "          \"\"\"\n",
    "          removing all hashtags , punctuations, stop_words  and links, also stemming words \n",
    "          \"\"\"\n",
    "          from nltk.corpus import stopwords\n",
    "          txt = \" \".join([self.camel_case_split(t) for t in txt.split(\" \")])\n",
    "          txt = re.sub(r\"(?<=\\w)nt\", \"not\",txt) #change don't to do not cna't to cannot \n",
    "          txt = re.sub(r'\\W', ' ', str(txt)) # remove all special characters including apastrophie \n",
    "          txt = txt.translate(str.maketrans('', '', string.punctuation)) # remove punctuations \n",
    "          txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)   # remove all single characters (it's -> it s then we need to remove s)\n",
    "          txt = re.sub(r'\\s+', ' ', txt, flags=re.I) # Substituting multiple spaces with single space\n",
    "          txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links \n",
    "          return txt\n",
    "\n",
    "\n",
    "      def camel_case_split(self, str):\n",
    "        words = [[str[0]]]\n",
    "\n",
    "        for c in str[1:]:\n",
    "            if words[-1][-1].islower() and c.isupper():\n",
    "                words.append(list(c))\n",
    "            else:\n",
    "                words[-1].append(c)\n",
    "\n",
    "        return \" \".join([''.join(word) for word in words])\n",
    "\n",
    "\n",
    "      def get_Entities(self, text):\n",
    "          text = self.clean_text(text)\n",
    "          doc = self.nlp_updated(text)\n",
    "          labels = [(X.text, X.label_) for X in doc.ents]\n",
    "\n",
    "          doc = self.nlp(text)\n",
    "          labels_norm = [(X.text, X.label_) for X in doc.ents]\n",
    "          labels.extend(labels_norm)\n",
    "\n",
    "          return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_complain(text):\n",
    "    bi = BinaryInference()\n",
    "    return str(bi.predict_complaint(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_complain_type(text):\n",
    "    multi = MulticlassComplainInference()\n",
    "    return str(multi.predict_complaint_type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_entities(text):\n",
    "    name_entity = NameEntities()\n",
    "    return json.dumps(dict(name_entity.get_Entities(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def reply_to_tweet(tweetId, complain, complain_type, entities, user):\n",
    "    # api-endpoint\n",
    "    URL = \"http://localhost:5000\"\n",
    "\n",
    "    # defining a params dict for the parameters to be sent to the API\n",
    "#     PARAMS = {'tweetId':tweetId, \"complain\" : complain, \"complain_type\" : complain_type, \"entities\" : entities, \"user\" : user}\n",
    "\n",
    "    # sending get request and saving the response as response object\n",
    "#     r = requests.post(url = URL, data = PARAMS)\n",
    "\n",
    "    # extracting data in json format\n",
    "#     data = r.json()\n",
    "    data = {\"replied\" : True, \"tickedId\" : 12345}\n",
    "    if data['replied']:\n",
    "        return str(data[\"tickedId\"])\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(words):\n",
    "    # complaint detection udf\n",
    "    check_complain_udf = udf(check_complain, StringType())\n",
    "    words = words.withColumn(\"complain\", check_complain_udf(col(\"tweet\")))\n",
    "    \n",
    "    # complaint category udf\n",
    "    check_complain_type_udf = udf(check_complain_type, StringType())\n",
    "    words = words.withColumn(\"complain_type\", check_complain_type_udf(col(\"tweet\")))\n",
    "    \n",
    "    # entity detection udf\n",
    "    get_name_entities_udf = udf(get_name_entities, StringType())\n",
    "    words = words.withColumn(\"entities\", get_name_entities_udf(col(\"tweet\")))\n",
    "    \n",
    "    reply_to_tweet_udf = udf(reply_to_tweet, StringType())\n",
    "    words = words.withColumn(\"tickedId\", reply_to_tweet_udf(col(\"tweet_id\"), col(\"complain\"), col(\"complain_type\"), col(\"entities\"), col(\"user\")))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------------+\n",
      "|               tweet|     user|           tweet_id|\n",
      "+--------------------+---------+-------------------+\n",
      "|RT @Bob_in_NYorks...|perrysmum|1407792704029868041|\n",
      "+--------------------+---------+-------------------+\n",
      "\n",
      "root\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- tweet_id: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+---------+-------------------+\n",
      "|tweet                                                                                                                                       |user     |tweet_id           |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+---------+-------------------+\n",
      "|RT @Bob_in_NYorks: It’s been said many times, but I’ll repeat. Train drivers and airline pilots are drink and drug tested. Why not MPs, who…|perrysmum|1407792704029868041|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+---------+-------------------+\n",
      "\n",
      "+--------------------+---------+-------------------+--------+-------------------+--------------------+--------+\n",
      "|               tweet|     user|           tweet_id|complain|      complain_type|            entities|tickedId|\n",
      "+--------------------+---------+-------------------+--------+-------------------+--------------------+--------+\n",
      "|RT @Bob_in_NYorks...|perrysmum|1407792704029868041|    True|Seating Preferences|{\"Train\": \"ORG\", ...|   12345|\n",
      "+--------------------+---------+-------------------+--------+-------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName('SampleTWEETOperation').getOrCreate()\n",
    "\n",
    "    #read json from text file\n",
    "    dfFromTxt=spark.read.text(\"my_data.txt\")\n",
    "\n",
    "    lines = dfFromTxt.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "\n",
    "    lines = lines.withColumn(\"jsonData\", from_json(col(\"json\"), jsonSchema))\\\n",
    "                   .select(\"jsonData.*\")\n",
    "    lines.show()\n",
    "    lines.printSchema()\n",
    "    lines.show(truncate=False)\n",
    "    \n",
    "    lines = label_data(lines)\n",
    "    lines.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
